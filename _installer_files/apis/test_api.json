{
  "6": {
    "inputs": {
      "text": "a video where the camera moves in a physically realistic manner, towards the tree. the camera moves rapidly, reaching the tree within 3 seconds",
      "clip": [
        "38",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Positive Prompt)"
    }
  },
  "7": {
    "inputs": {
      "text": "Vibrant colors, overexposed, static, blurry details, subtitles, style, artwork, painting, image, still, overall grayish, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, distorted limbs, fingers fused together, static image, cluttered background, three legs, many people in the background, walking backwards.",
      "clip": [
        "38",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Negative Prompt)"
    }
  },
  "8": {
    "inputs": {
      "samples": [
        "58",
        0
      ],
      "vae": [
        "39",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "38": {
    "inputs": {
      "clip_name": "umt5_xxl_fp8_e4m3fn_scaled.safetensors",
      "type": "wan",
      "device": "cpu"
    },
    "class_type": "CLIPLoader",
    "_meta": {
      "title": "Load CLIP"
    }
  },
  "39": {
    "inputs": {
      "vae_name": "wan_2.1_vae.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "50": {
    "inputs": {
      "width": [
        "64",
        1
      ],
      "height": [
        "64",
        2
      ],
      "length": 81,
      "batch_size": 1,
      "positive": [
        "6",
        0
      ],
      "negative": [
        "7",
        0
      ],
      "vae": [
        "39",
        0
      ],
      "start_image": [
        "64",
        0
      ]
    },
    "class_type": "WanImageToVideo",
    "_meta": {
      "title": "WanImageToVideo"
    }
  },
  "54": {
    "inputs": {
      "shift": 8.000000000000002,
      "model": [
        "69",
        0
      ]
    },
    "class_type": "ModelSamplingSD3",
    "_meta": {
      "title": "ModelSamplingSD3"
    }
  },
  "55": {
    "inputs": {
      "shift": 8.000000000000002,
      "model": [
        "70",
        0
      ]
    },
    "class_type": "ModelSamplingSD3",
    "_meta": {
      "title": "ModelSamplingSD3"
    }
  },
  "57": {
    "inputs": {
      "add_noise": "enable",
      "noise_seed": 742434422083747,
      "steps": 6,
      "cfg": 1,
      "sampler_name": "euler",
      "scheduler": "simple",
      "start_at_step": 0,
      "end_at_step": 3,
      "return_with_leftover_noise": "enable",
      "model": [
        "54",
        0
      ],
      "positive": [
        "50",
        0
      ],
      "negative": [
        "50",
        1
      ],
      "latent_image": [
        "50",
        2
      ]
    },
    "class_type": "KSamplerAdvanced",
    "_meta": {
      "title": "KSampler (Advanced)"
    }
  },
  "58": {
    "inputs": {
      "add_noise": "disable",
      "noise_seed": 0,
      "steps": 6,
      "cfg": 1,
      "sampler_name": "euler",
      "scheduler": "simple",
      "start_at_step": 3,
      "end_at_step": 10000,
      "return_with_leftover_noise": "disable",
      "model": [
        "55",
        0
      ],
      "positive": [
        "50",
        0
      ],
      "negative": [
        "50",
        1
      ],
      "latent_image": [
        "57",
        0
      ]
    },
    "class_type": "KSamplerAdvanced",
    "_meta": {
      "title": "KSampler (Advanced)"
    }
  },
  "61": {
    "inputs": {
      "unet_name": "wan2.2_i2v_high_noise_14B_Q4_K_S.gguf"
    },
    "class_type": "UnetLoaderGGUF",
    "_meta": {
      "title": "Unet Loader (GGUF)"
    }
  },
  "62": {
    "inputs": {
      "unet_name": "wan2.2_i2v_low_noise_14B_Q4_K_S.gguf"
    },
    "class_type": "UnetLoaderGGUF",
    "_meta": {
      "title": "Unet Loader (GGUF)"
    }
  },
  "63": {
    "inputs": {
      "frame_rate": 16,
      "loop_count": 0,
      "filename_prefix": "2025-12-02/wan22_",
      "format": "video/h264-mp4",
      "pix_fmt": "yuv420p",
      "crf": 19,
      "save_metadata": true,
      "trim_to_audio": false,
      "pingpong": false,
      "save_output": true,
      "images": [
        "8",
        0
      ]
    },
    "class_type": "VHS_VideoCombine",
    "_meta": {
      "title": "Video Combine ðŸŽ¥ðŸ…¥ðŸ…—ðŸ…¢"
    }
  },
  "64": {
    "inputs": {
      "width": 480,
      "height": 480,
      "upscale_method": "lanczos",
      "keep_proportion": "crop",
      "pad_color": "0, 0, 0",
      "crop_position": "center",
      "divisible_by": 16,
      "device": "cpu",
      "image": [
        "80",
        0
      ]
    },
    "class_type": "ImageResizeKJv2",
    "_meta": {
      "title": "Resize Image v2"
    }
  },
  "65": {
    "inputs": {
      "sage_attention": "sageattn_qk_int8_pv_fp8_cuda++",
      "model": [
        "61",
        0
      ]
    },
    "class_type": "PathchSageAttentionKJ",
    "_meta": {
      "title": "Patch Sage Attention KJ"
    }
  },
  "66": {
    "inputs": {
      "enable_fp16_accumulation": true,
      "model": [
        "65",
        0
      ]
    },
    "class_type": "ModelPatchTorchSettings",
    "_meta": {
      "title": "Model Patch Torch Settings"
    }
  },
  "67": {
    "inputs": {
      "sage_attention": "sageattn_qk_int8_pv_fp8_cuda++",
      "model": [
        "62",
        0
      ]
    },
    "class_type": "PathchSageAttentionKJ",
    "_meta": {
      "title": "Patch Sage Attention KJ"
    }
  },
  "68": {
    "inputs": {
      "enable_fp16_accumulation": true,
      "model": [
        "67",
        0
      ]
    },
    "class_type": "ModelPatchTorchSettings",
    "_meta": {
      "title": "Model Patch Torch Settings"
    }
  },
  "69": {
    "inputs": {
      "lora_name": "high_noise_model.safetensors",
      "strength_model": 3.0000000000000004,
      "model": [
        "66",
        0
      ]
    },
    "class_type": "LoraLoaderModelOnly",
    "_meta": {
      "title": "LoraLoaderModelOnly"
    }
  },
  "70": {
    "inputs": {
      "lora_name": "low_noise_model.safetensors",
      "strength_model": 1.5000000000000002,
      "model": [
        "68",
        0
      ]
    },
    "class_type": "LoraLoaderModelOnly",
    "_meta": {
      "title": "LoraLoaderModelOnly"
    }
  },
  "80": {
    "inputs": {
      "image": "unnamed.jpg"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  }
}